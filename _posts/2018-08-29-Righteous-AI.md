---
layout: post
title: Righteous AI
categories: Tech
---
In an increasingly digital world, data analytics offer competitive and attractive advantages for creating efficiency and value. Companies see the big $$$ from this and scramble to create the next big AI system to interpret data. Yet there remains a responsibility to ensure that these systems are fair, accountable, and transparent. 

<!--more-->

AI has the ability to completely dictate people’s lives, for good and for worse. Education, employment, finances, healthcare, and pretty much any facet of our life that can be compartmentalized into databases has the potential to be modeled by smart computers. There have been many instances where AI has done a lot of good, like the virtual companionship and wisdom of Siri, or the stress-free travel ensured by Google Maps. But, there has also been cases where AI has effectively ruined people’s lives, like micro-targeted banking services models that refuse loans to the poor making them resort to higher interest alternatives, or crime detecting software that unequivocally and unfairly focuses on certain minorities in poorer areas. Our aim should be more of the former and less of the latter.

More often than not, the poor and the minorities are the ones who suffer from the hands of discriminatory AI and they are the ones least able to afford it.

### Fair, Accountable, Transparent:

To be fair means different things for different people, yet there is a universal agreement that algorithms should not discriminate. Data may be naturally biased, or the proxies chosen by developers to use in models may be inappropriate, or the algorithms may be naive in implementation. Whatever the case, the creators of these models have a duty to pursue equality. If we cannot achieve this, then systems that are supposed to ameliorate problems may end up exacerbating them. What makes this so difficult is that there is no consistent benchmark for determining fairness; everything depends on the circumstances. For example, in hiring practices, considering gender would not be fair to the people applying. However, in the detection of breast cancer, gender would make a huge difference in saving those in need.

Designers and developers of these systems must be held accountable and share responsibility for the consequences of their creations. Acknowledging that something went wrong is the first step towards beneficial change. This involves asking hard questions and taking on even harder challenges. Oftentimes, the engineers feel it is the duty of management to address the consequences of their work; they are merely the pawns who implement plans from higher up. Meanwhile, management may be unaware of these issues. Communication would be crucial in this call for accountability.

Finally, transparency refers to bringing understanding and interpretation to the inner workings of AI. Many systems are currently seen as a “black box”, where the procedures and decision-making are unclear, potentially even to those who designed them. If we do not understand how these AI systems work, how can we trust it to be in charge of driving a car, diagnosing illness, or holding sway over a person’s job or prison sentence? If we do not understand how these AI systems work, how can we even begin fixing them?

### Practicing practicalities:

I propose that we will need a strong regulatory presence, scrutinous press coverage, and companies/consumers who understand the need for FAT (fair, accountable, transparent) AI.

Regulators can keep companies in line and punish those who choose efficiency over equality, forcing them to take responsibility. We must make sure that those who pass the regulations, the politicians, have in their interest the general good of the population. This will be crucial but also difficult, because the government operates slowly. Vigilant press coverage can expose the misdeeds of corporations and shame them into changing. Press can also generate awareness in companies and consumers leading to a community that understands the need for and advocates for transparent and fair algorithms. This, I believe, is the most essential step. If we all push for change, then it will be certain to happen.

As author and mathematician Cathy O’Neil puts it, “big data models codify the past, they do not account for the future.” It is time for us to begin questioning the methods of big data. Our algorithms must learn to overcome existing societal biases, and more importantly, not create any new ones.
