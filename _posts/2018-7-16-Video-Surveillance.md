---
layout: post
title: The Dystopian Future Nobody Wants
categories: Tech
---
Machine learning is hot hot hot and it was simply a matter of time before the camera industry hopped on that hypetrain fast fast fast. 
<!--more-->

According to Wikipedia, an authoritative source, machine learning is a subset of Artificial Intelligence that allow computers to learn from data without being explicitly programmed. 

To put it simply, it allows computers to learn and understand from experiences like a human should. Pair that up with cameras and we suddenly have cameras that ostensibly see like humans.

We see it in Apple’s newest iPhone X, in Facebook’s tagging suggestions, and countless other products. Google recently released Google Clips, an extraordinary AI powered camera that seems to accomplish many of the inhuman wonders expected of today’s machines. It uses machine learning techniques to automatically capture whatever it deems “interesting”, by responding to lighting, facial expressions, and other aspects of a good photograph. This product is truly revolutionary and is just one of the many products meant to transform the camera/video/entertainment industry. 

Since beauty is a very abstract concept, it is incredible that we can possibly program a machine to understand a concept that we have difficulty describing.  

## Why is this product so cool? ##
To use the camera, you simply turn it on and leave it pointing in the general target vicinity; all of the photography happens automatically. It can easily capture tons of organic and candid photos without much human effort, and even allows people to momentarily forget about their dreadful fear of the filming process! No longer will parents have to force their children to act natural in front of an eeringly weapon-like object shaped like a camera. No longer will children have to worry about if they are awkwardly smiling when adults tell them to “Smile!” Furthermore, all of the data from the camera is stored locally, so privacy is protected and ensured against harmful agents like the government. Ha. I admit I have quite lofty expectations for this camera and am concerned about my expectations not being met. Like, all photos of me MUST be good looking because I will accept nothing less than the truthful depiction of my photogenic self. But I am also sure that there are far greater worries to fret about... 


![GoogleClips](/public/images/GoogleClips.jpg){:class="img-responsive" height="366px" width="550px"}

****

Moving on to a more serious note, a major use for these smart-cameras is in **security and surveillance.** Tech companies are developing surveillance camera software that can recognize faces and the dangerous behaviors performed by the owners of those faces. This could help reduce crime rates by identifying criminals and other lawbreakers. Even the perception of smart surveillance, even if it isn’t actually functional, can work to deter crimes. 

## This innovative technology is exciting and has multitudes of applications and potential, but at what cost? ##

Privacy issues raise an obvious concern. People will no doubt oppose being captured on camera without their consent and rightfully so. Information gleaned from video surveillance is extremely valuable and can be used to manipulate people, whether for personalized advertisements or worse, malicious intents. One can easily learn about a person’s habits, location and other important personal information. Just look at all of the lawsuits with Facebook recently over their facial recognition and tagging feature to see the outrage that people demonstrate over privacy violations. 

Tech companies have tried to bring more transparency towards their data-collecting methods but this in itself is not a solution to the privacy issue. Allowing users to toggle these invasive features does not aptly count either. What needs to happen is some form of valuation over the worth of data and appropriate compensation so that corporate forces no longer exploit our information in the process of building their profit-inflating products. We need to think carefully about giving away our data for free and we need stringent government regulation to keep these companies in check.

A less visible risk includes potential bias towards targeted groups of people by the smart-cameras. These cameras are trained on data-sets, and if these data-sets are biased then the cameras will inherit this behavior. For example, if a machine learns from recordings that show police officers exhibiting racial or sexist prejudices, it will for sure soak up these behaviors. And this sort of bias can lead to a wrongful conviction or far worse. This is a sensitive topic for all areas of machine learning and in the age of big data, it is exceedingly important that we choose and interpret our data appropriately. 

Cameras cannot understand the world as well as humans do, and it becomes an ethical problem when we assume that they can and allow them to make decisions for us.

## Let's look at China. Hmm ##

China has been investing in surveillance companies very heavily and plans to install a mass surveillance system that can ultimately be used to control its citizens. Imagine a dystopian society where the government watches your every move and secretly removes all traces of opposition. Now magnify that level of frightening many times over and that will be China in the future, since real life is of course scarier than fiction. Their goal has been quoted to be “algorithmic dominance”, which is as alarming as it sounds. It is clear why this sort of technology is so dangerous and why regulation over this technology remains crucial. 


![SurveillanceGif](/public/images/ai_china_surveillance.gif){:class="img-responsive"}


In some parts of China, this fear has already become palpable. The Uyghur minority in Xinjiang, China have been facing high-tech surveillance and monitoring technology since their riots in 2009. From social media monitoring to dna samples to now facial recognition tracking software, the Islamic Uyghur population are involuntarily forced to participate in religious re-education systems and every moment of their life is controlled. 

According to The New York Times, "When Uighurs buy a kitchen knife, their ID data is etched on the blade as a QR code." BuzzFeed documented stories of family members too scared to speak openly to relatives abroad. As a result, riots, protests, and violence have fallen in Xinjiang thanks to the fear instilled by the government. This was the first sign of a large-scale shift in tactics by the Chinese using a powerful tool: technological control over information. Inspired by their “success”, China now aims to implement this over its entire, humongous landmass. Unfortunately, it is not just China that is determined to invest in video/camera and AI. Western powers --companies and governments-- seek this control over data as well, to a lesser extent.

## How do we solve this? ##

To me, effective regulation over video surveillance/facial recognition technology is a grandiose ambition, perhaps even too ambitious. In the current world, dominated by those with capitalist agendas and with the data of the many in the hands of the few, it may be all but possible to achieve this state of equilibrium between continued innovation and user protection. Still, I remain optimistic that we can come close to a solution. 

For now, I will only focus on the US, because it is the most relevant to me. 

I argue that we must first start with setting the right tone towards this technology in the government. For, it is the government that is in control of regulating itself and the businesses involved with this technology, and it is the government that we, the people, have a voice in. We can let it be known that AI-camera issues matter dearly to us. And we can pressure the government to introduce laws that increase transparency, give users more control, protect privacy, and foil capitalist exploitations. 

It is important to choose the right people for this job and perhaps a special committee is needed to further investigate these matters. Currently, the Federal Trade Commission is in charge of protecting user privacy, but it is unclear if they are right for the job. The reason is that the main purpose of the FTC is to deal with “unfair and deceptive” business practices rather than deal with specific privacy concerns. So, they may be hesitant in exercising power over issues that are not outright illegal, which many privacy matters fall into. 

Another reason for government intervention is that we cannot rely on the tech industry to choose the protection of users over profits. Some might argue that it is in the tech industry's interest to self-regulate privacy issues. After all, they may fear losing customers if they continue to use and gather private data inappropriately from its users. However as of right now, many of the self-regulation schemes they have created only address how data is used, and not how it is collected in the first place. This is simply not enough and we should be fearful of companies knowing so much about individuals. 

A common attitude in the tech industry towards abuse of technologies is that the technologies themselves are value neutral, and that it is how they are implemented and by whom that determines if they are good or bad. In other words, they refuse to take responsibility. 

## What will our future look like? ##

The future remains bright in the face of the AI revolution and the spread of AI into camera/video. Our world is improving thanks to human innovation and ingenuity and has never looked as good as it has today. People are safer, healthier, living longer, and spending a good chunk of their time interacting with different forms of media powered by cameras. 

There are many issues concerning this rapid innovation that may cast a shadow on the progress of AI, but with an appropriate and unified approach, we can mitigate these shadows and provide a brighter future.
